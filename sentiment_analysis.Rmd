---
title: "Sentiment Analysis of Bruce Springsteen's Lyrics"
author: "Alessandro Montanari"
date: "15 January 2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***

## 1. Introduction

Bruce Springsteen is one of the most popular rockstars in the world. His career spans over 5 decades, from 1973 to 2020, counting 20 studio albums produced and over 150 million records sold worldwide.  
  
The aim of this project will be to analyze Springsteen's lyrics in order to extract the most commonly used terms and the feelings expressed through his verses. In order to do so, the lyrics of his songs will be downloaded, cleaned of punctuation and stop-words, and finally lemmatized, producing a term-document matrix which will allow the visualization of the results through a word-cloud. Afterwards, a *Sentiment Analysis* will be carried out, extracting the emotions and assigning an average sentiment score to each song.  
Firstly, the analysis will be carried out on Springsteen's whole discography, to obtain the general results, and secondly on a decade-to-decade basis in order to examine his evolution through the years.
  
### 1.1 The Data  

The data consists of a *Corpus* of *txt* files, containing the lyrics of every song recorded by Bruce Springsteen. The lyrics were obtained by web-scraping **[AZlyrics.com](https://www.azlyrics.com)** through a Python function that I wrote, and that can be more generally used for downloading the lyrics of any artist or single song on the website as a *txt* file. You can find the code **[here](https://github.com/a-montanari/webscraping_lyrics)**, along with some general instructions on how to use the function.  
The total number of songs is 369, although only the 273 studio album songs were considered when studying the decade-by-decade lyrics evolution. A list of all the songs can be obtained **[here](https://www.azlyrics.com/s/springsteen.html)**.

### 1.2 Required Packages

```{r requirements, results='hide', message=FALSE}
require(tm)
require(SnowballC)
require(wordcloud2)
require(wordcloud)
require(RColorBrewer)
require(syuzhet)
require(ggplot2)
require(textstem)
require(knitr)
require(kableExtra)
require(htmlwidgets)
require(webshot)
```

***

## 2. Full Discography Analysis (1973-2020)
### 2.1 Importing and cleaning the lyrics

The lyrics were imported in a list of documents through the `Corpus` function, with each element of the list corresponding to a different song.
  
```{r importing, warning=FALSE}
setwd("~/songs/all")
names <- list.files()
text <- lapply(names, readLines)
Lyrics <- Corpus(VectorSource(text))
substr(Lyrics[[1]]$content, 1, 117)
```
  
  
Above are printed the three initial verses of the first song: *4th of July, Asbury Park (Sandy)*. Although this is actually one of Springsteen's first songs, note that the list is in alphabetical order and not chronological.  
It is clear that the text needs to be cleaned before the analysis. In particular, we could consider:
  
- Converting the text to lower case
- Removing the numbers
- Removing the common stop-words
- Removing the punctuation and the extra white spaces
- Correcting the abbreviations, such as *hailin'* to *hailing* and *forcin'* to *forcing*
- *Lemmatizing* the words by reducing them to their common base form
  
A function `clean_lyrics` was created to facilitate the later steps of the analysis. This takes as parameters:  
  
- `Lyrics`: a corpus of characters
- `remove_sw`: boolean; if `TRUE`, common English stopwords are removed (`FALSE` by default)
- `my_sw`: an additional array of personalized stopwords to be removed
- `lemmatize`: boolean; if `TRUE`, the words are lemmatized (reduced to their base form) and common English stopwords are removed (`FALSE` by default)
  
```{r cleaning, message=FALSE, warning=FALSE}

clean_lyrics <- function(Lyrics, remove_sw = FALSE, my_sw = NULL, lemmatize = FALSE){
  # Correcting abbreviations
  correct_verb <- content_transformer(function (x , pattern) gsub(pattern, "ing", x))
  Lyrics <- tm_map(Lyrics, correct_verb, "in'")
  # Converting the text to lower case
  Lyrics <- tm_map(Lyrics, content_transformer(tolower))
  # Removing the numbers
  Lyrics <- tm_map(Lyrics, removeNumbers)
  if(remove_sw==TRUE | lemmatize == TRUE){
    # Removing English common stop-words + "come on"
    Lyrics <- tm_map(Lyrics, removeWords, c(stopwords("english"), "come on"))
  }
  # Removing punctuation and correcting the "curly" apostrophe
  straight_apo <- content_transformer(function (x, pattern) gsub(pattern, "'", x))
  Lyrics <- tm_map(Lyrics, straight_apo, "â€™")
  Lyrics <- tm_map(Lyrics, removePunctuation)
  # Eliminating extra white spaces
  Lyrics <- tm_map(Lyrics, stripWhitespace)
  # Removing personalized stop-words
  Lyrics <- tm_map(Lyrics, removeWords, my_sw)
  Lyrics <- tm_map(Lyrics, removeWords, "c")
  if(lemmatize == TRUE){
    Lyrics <- tm_map(Lyrics, lemmatize_strings)
  }
  return(Lyrics)
}
# These stopwords were found through a preliminary analysis of the lyrics
mystopwords <- c("now", "aint", "can", "will", "whoa", "hey", "gonna",
                 "yeah", "just", "got", "wanna", "aingt", "sha")

Lyrics_cleaned <- clean_lyrics(Lyrics)
Lyrics_sw <- clean_lyrics(Lyrics, remove_sw = TRUE, my_sw = mystopwords)
Lyrics_lemmatized <- clean_lyrics(Lyrics, lemmatize = TRUE, my_sw = mystopwords)
```

  
The code above yields 3 different lists:

- **Lyrics_cleaned**, containing the original text cleaned of punctuation, white spaces and numbers;
- **Lyrics_sw**, where additionally the stop-words are removed;
- **Lyrics_lemmatized**, where the words are also reduced to their base form.

In order to explore the differences, let us examine the initial verses of *4th of July, Asbury Park (Sandy)* after the transformations:

```{r text_comparison, comment=NA}
substr(Lyrics_cleaned[[1]]$content, 2, 100) # Cleaned text
substr(Lyrics_sw[[1]]$content, 2, 70) # Text without stop-words
substr(Lyrics_lemmatized[[1]]$content, 1, 62) # Lemmatized text
```
  
  
### 2.2 Term-Document Matrix and Word Cloud
  
The next step of the analysis consists of building a *Term-Document Matrix* starting from the cleaned lyrics. This contains the count of the repetitions of each unique word in each of the lyrics.  
As shown by the dimensions of the matrix printed below, the Corpus (cleaned of the stop-words) is made up of 5680 unique words over 369 documents. By summing each row, we can get how many times each term is repeated over Springsteen's whole discography, and extract the most common ones by building a table.
  

```{r tdm, out.width="50%"}
Lyrics_dtm <- TermDocumentMatrix(Lyrics_sw)
dtm_m <- as.matrix(Lyrics_dtm)
dim(dtm_m) # Dimensions of the term-document matrix
dtm_v <- sort(rowSums(dtm_m), decreasing = TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq = dtm_v)
kable(head(dtm_d, 10), col.names = c("Word", "Frequency"), row.names = FALSE,
      caption = "Table 1: Most Common Terms (Cleaned Text)", align = "c") %>%
  kable_styling(full_width = F)
```

If we exclude the stop-words, Springsteen's favorite word turns out to be *baby*, with 513 repetitions over 369 songs, followed by *well*, *night* and *love*.  
The Term-Document Matrix can be used to build a **word cloud** for a nice visualization of the results.
  
```{r wordcloud1, warning=FALSE, fig.align="center", fig.width=5, fig.cap="Figure 1: Cleaned Text Word Cloud - Springsteen's whole discography"}
wc <- wordcloud2(dtm_d, fontFamily = "Arial", size = 1.2)
saveWidget(wc,"wordclouds/1.html", selfcontained = F)
webshot("wordclouds/1.html", "wordclouds/1.png", vwidth = 960, vheight = 628, delay = 20)
```

If we consider the lemmatized text instead of the original one, the unique terms are only 4183, against the 5680 original ones. This is to be expected, since the purpose of the lemmatization is exactly to reduce different words to the same base form.


```{r tdm_lemmatized}
Lyrics_dtm <- TermDocumentMatrix(Lyrics_lemmatized)
dtm_m <- as.matrix(Lyrics_dtm)
dim(dtm_m) # Dimensions of the term-document matrix
dtm_v <- sort(rowSums(dtm_m), decreasing = TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq = dtm_v)
kable(head(dtm_d,10), col.names = c("Lemma", "Frequency"), row.names = FALSE,
      caption = "Table 2: Most Common Terms (Lemmatized Text)", align = "c") %>%
  kable_styling(full_width = F)
```
The most common lemmatized term is *good*, followed by *come*, *night* and *baby*.  
These results can once again be represented as a word cloud:

```{r wordcloud_lemmatized, fig.align="center", fig.width=5, fig.cap="Figure 2: Lemmatized Text Word Cloud - Springsteen's whole discography"}
wc <- wordcloud2(dtm_d, fontFamily = "Arial", size = 1.5)
saveWidget(wc,"wordclouds/2.html", selfcontained = F)
webshot("wordclouds/2.html", "wordclouds/2.png", vwidth = 960, vheight = 628, delay = 20)
```

### 2.3 Sentiment Analysis
  
The function `get_nrc_sentiment` of the `syuzhet` package implements the *NRC Emotion Lexicon* to the text, which associates specific words to one of **eight emotions**:  
  
1. Anger
2. Fear
3. Anticipation
4. Trust
5. Surprise
6. Sadness
7. Joy
8. Disgust

In particular, the function returns a data frame in which each row represents a string of text, and each column the number of words contained by that string associated with each emotion.  
In order to apply the function to our lyrics, we first need to build a vector containing the text (the function cannot be applied to a list). To this purpose, the lemmatized text will be used.

```{r textfull}
text_full_lemmatized <- c()
for(i in 1:length(Lyrics_lemmatized)){
  text_full_lemmatized <- append(Lyrics_lemmatized[[i]]$content, text_full_lemmatized)
}
```

```{r nrc_sentiments}
d <- get_nrc_sentiment(text_full_lemmatized)
kable(head(d,10)[,1:8],
      caption = "Table 3: Count of words associated to each emotion (first 10 lyrics, with each row representing a different song)",
      col.names = c("Anger", "Anticipation", "Disgust", "Fear", "Joy", "Sadness", "Surprise", "Trust"), align = "c") %>%
  kable_styling(full_width = F)
```
By summing the columns of the whole sentiments data frame and sorting them, we can obtain the count of each emotion in the whole collection of lyrics, from most to least present.  
For Springsteen's discography, it can be observed that the most recurrent emotions are, in order, *joy*, *trust* and *anticipation*, whereas terms associated with *sadness*, *fear* and *anger* are generally less used common. In other words, we could say that his lyrics are more often positive than negative.

```{r emotions_plot, fig.align="center", out.width="65%", fig.cap="Figure 3: Count of the terms associated with each of the 8 sentiments for Springsteen's whole discography"}
td<-data.frame(t(d))
td_new <- data.frame(rowSums(td))
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
# Count of words associated with each sentiment
sentiments <- td_new2$sentiment
ord <- order(td_new2$count, decreasing = T)
ord_sent <- td_new2[ord,]
rownames(ord_sent) <- NULL
ord_sent$sentiment <- factor(ord_sent$sentiment, levels = sentiments[ord])
quickplot(sentiment, data = ord_sent, weight = count, geom = "bar",
          fill = sentiment, ylab = "Count", xlab = "Sentiment")+
  ggtitle("Springsteen's Lyrics sentiments - Whole Discography (1973-2020)") +
  theme(legend.position = "none")
```

To examine more in depth the sentiment score (meaning, *positive* or *negative*) of the lyrics, the `get_sentiment` function can be very helpful. 
This function, applied to a list of sentences, returns scores of the sentiment of each phrase. A score greater than 0 implies that the sentence is associated with a positive meaning, and vice versa. Summary statistics of the scores can be eventually computed to assess the general sentiment of the text.  
In the case of lyrics, we can apply the function to each (original) verse, compute an average score for each song, and finally examine the distribution of these average scores over the whole discography with, for instance, a boxplot.
  
```{r avgscore}
avg_score = numeric(0)
lyrics_score = list()
for(i in 1:length(text)){
  lyrics_score[[i]] <- get_sentiment(text[[i]][text[[i]]!=""], method="syuzhet")
  avg_score[i] <- mean(lyrics_score[[i]])
}
```

```{r boxplot, fig.align="center", out.width="50%", fig.asp=1, fig.cap="Figure 4: Sentiment Score Distribution - Springsteen's whole discography"}
boxplot(avg_score, col = "skyblue")
abline(h = 0, col = "red", lwd = 2)
```

```{r distsum}
kable(t(summary(avg_score)), caption = "Table 4: Sentiment Score distribution summary", digits = 4, align = "c") %>%
  kable_styling(full_width = F)
```

Indeed, the great majority of songs have a positive average score: almost 3/4 are greater than 0.  

***

## 3. Decade by Decade Analysis
  
In order to examine the evolution of the lyrics over time, each decade was analyzed separately, presenting the lemmatized word clouds along with a plot showing the average sentiment score distribution of each song.
  
### 3.1 The 1970s  
  
```{r 1970s_importing_and_cleaning, message=FALSE, warning=FALSE}
setwd("~/songs/decades/1970s")
names70 <- list.files()
text70 <- lapply(names70, readLines)
Lyrics70 <- Corpus(VectorSource(text70))
Lyrics70_lemmatized <- clean_lyrics(Lyrics70, lemmatize = TRUE, my_sw = mystopwords)
```

```{r tdm_1970}
# Lemmatized Text
Lyrics70_dtm <- TermDocumentMatrix(Lyrics70_lemmatized)
dtm_m <- as.matrix(Lyrics70_dtm)
dtm_v <- sort(rowSums(dtm_m), decreasing = TRUE)
dtm70_lm <- data.frame(word = names(dtm_v), freq = dtm_v)
```

```{r wordclouds_70s, warning=FALSE, fig.width=5, fig.align="center", fig.cap="Figure 5: 1970s Lemmatized Word Cloud"}
cols <- brewer.pal(9, 'Blues')
newcol <- colorRampPalette(cols)
ncols <- 500
cols2 <- newcol(ncols)
cols2 <- cols2[470:1]
wc <- wordcloud2(dtm70_lm, fontFamily = "Arial", size = 2, color = cols2)
saveWidget(wc,"wordclouds/70.html", selfcontained = F)
webshot("wordclouds/70.html", "wordclouds/70.png", vwidth = 960, vheight = 628, delay = 15)
```

```{r sentimet_score70s, out.width="75%", fig.align="center", fig.cap="Figure 6: Average Sentiment Score for each song from the 1970s"}
avg_score70 = numeric(0)
lyrics_score70 = list()
for(i in 1:length(text70)){
  lyrics_score70[[i]] <- get_sentiment(text70[[i]][text70[[i]]!=""], method="syuzhet")
  avg_score70[i] <- mean(lyrics_score70[[i]])
}
plot(avg_score70, type = "l", ylab = "Average Sentiment Score", xlab = "",
     main = "Songs from 1970-1979")
abline(h=0, col="red")
points(x = c(which.min(avg_score70), which.max(avg_score70), 3),
       y = c(min(avg_score70), max(avg_score70), avg_score70[3]),
       pch = 21, col = "black", bg = "red")
text(x = c(which.min(avg_score70)-4.6, which.max(avg_score70)-5, 6.8),
     y = c(min(avg_score70), max(avg_score70), avg_score70[3]),
     labels = c("Streets Of Fire", "Prove It All Night", "Backstreets"))
```
Springsteen's songs from the 1970s are generally distributed around a neutral sentiment score of 0. *Backstreets* and *Streets of Fire* result as the ones conveying the most negative emotions, whereas *Prove it All Night* appears to be the most positive song from this decade according to the *Syuzhet* dictionary.  

### 3.2 The 1980s

```{r 1980s_importing_and_cleaning, message=FALSE, warning=FALSE}
setwd("~/songs/decades/1980s")
names80 <- list.files()
text80 <- lapply(names80, readLines)
Lyrics80 <- Corpus(VectorSource(text80))
Lyrics80_lemmatized <- clean_lyrics(Lyrics80, lemmatize = TRUE, my_sw = mystopwords)
```

```{r tdm_1980}
# Lemmatized Text
Lyrics80_dtm <- TermDocumentMatrix(Lyrics80_lemmatized)
dtm_m <- as.matrix(Lyrics80_dtm)
dtm_v <- sort(rowSums(dtm_m), decreasing = TRUE)
dtm80_lm <- data.frame(word = names(dtm_v), freq = dtm_v)
```

```{r wordclouds_80s, warning=FALSE, fig.width=5, fig.align="center", fig.cap="Figure 7: 1980s Lemmatized Word Cloud"}
cols <- brewer.pal(9, 'YlOrRd')
newcol <- colorRampPalette(cols)
ncols <- 600
cols2 <- newcol(ncols)
cols2 <- cols2[580:1]
wc <- wordcloud2(dtm80_lm, fontFamily = "Arial", size = 1.7, color = cols2)
saveWidget(wc,"wordclouds/80.html", selfcontained = F)
webshot("wordclouds/80.html", "wordclouds/80.png", vwidth = 960, vheight = 628, delay = 12)
```

```{r sentimet_score80s, out.width="75%", fig.align="center", fig.cap="Figure 8: Average Sentiment Score for each song from the 1980s"}
avg_score80 = numeric(0)
lyrics_score80 = list()
for(i in 1:length(text80)){
  lyrics_score80[[i]] <- get_sentiment(text80[[i]][text80[[i]]!=""], method="syuzhet")
  avg_score80[i] <- mean(lyrics_score80[[i]])
}
plot(avg_score80,
     type = "l", ylab = "Average Sentiment Score", xlab = "",
     main = "Songs from 1980-1989")
abline(h=0, col="red")
points(x = c(which.min(avg_score80), 27, which.max(avg_score80), 44, 53, 1),
       y = c(min(avg_score80), avg_score80[27], max(avg_score80),
             avg_score80[44], avg_score80[53], avg_score80[1]),
       pch = 21, col = "black", bg = "red")
text(x = c(which.min(avg_score80)-5, 19, which.max(avg_score80)-7, 33.5, 46.8, 7),
     y = c(min(avg_score80), avg_score80[27], max(avg_score80),
           avg_score80[44], avg_score80[53]+0.01, avg_score80[1]),
     labels = c("Stolen Car", "My Father's House", "Tunnel Of Love", "Tougher Than The Rest", "Wreck On The 
          Highway", "Ain't Got You"))

```

The 1980s songs are generally much more positive than negative, with the "best" scores being reached by *Tunnel Of Love*, *Tougher Than The Rest* and *Ain't Got You*. While the first two songs can be effectively defined as "positive", the lyrics of the latter are actually meant to be quite sad:

```{r aint_got_you_lyrics, comment=NA, warning=FALSE}
score_df <- data.frame(cbind(lyrics_score80[[1]], text80[1][[1]][text80[[1]]!=""]))
kable(score_df, col.names = c("Score", "Verse"),
      caption = "Ain't Got You (Tunnel Of Love, 1987) - Sentiment Score for each verse", align = "c") %>%
  kable_styling(full_width = F)
```
Even though the general meaning of the song is sad, the particular way in which it is written, listing all the "positive" aspects of the protagonist's life, yields an average sentiment score of 0.619.

### 3.3 The 1990s

```{r 1990s_importing_and_cleaning, message=FALSE, warning=FALSE}
setwd("~/songs/decades/1990s")
names90 <- list.files()
text90 <- lapply(names90, readLines)
Lyrics90 <- Corpus(VectorSource(text90))
Lyrics90_lemmatized <- clean_lyrics(Lyrics90, lemmatize = TRUE, my_sw = mystopwords)
```

```{r tdm_1990}
# Lemmatized Text
Lyrics90_dtm <- TermDocumentMatrix(Lyrics90_lemmatized)
dtm_m <- as.matrix(Lyrics90_dtm)
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm90_lm <- data.frame(word = names(dtm_v),freq=dtm_v)
```

```{r wordclouds_90s, warning=FALSE, fig.width=5, fig.align="center", fig.cap="Figure 9: 1990s Lemmatized Word Cloud"}
cols <- brewer.pal(9, 'Purples')
newcol <- colorRampPalette(cols)
ncols <- 600
cols2 <- newcol(ncols)
cols2 <- cols2[580:1]
wc <- wordcloud2(dtm90_lm, fontFamily = "Arial", size = 1.7, color = cols2)
saveWidget(wc,"wordclouds/90.html", selfcontained = F)
webshot("wordclouds/90.html", "wordclouds/90.png", vwidth = 960, vheight = 628, delay = 12)
```

```{r sentimet_score90s, out.width="75%", fig.align="center", fig.cap="Figure 10: Average Sentiment Score for each song from the 1990s"}
avg_score90 = numeric(0)
lyrics_score90 = list()
for(i in 1:length(text90)){
  lyrics_score90[[i]] <- get_sentiment(text90[[i]][text90[[i]]!=""], method="syuzhet")
  avg_score90[i] <- mean(lyrics_score90[[i]])
}
plot(avg_score90,
     type = "l", ylab = "Average Sentiment Score", xlab = "",
     main = "Songs from 1990-1999")
abline(h=0, col="red")
points(x = c(which.min(avg_score90), 5, which.max(avg_score90), 23, 10),
       y = c(min(avg_score90), avg_score90[5], max(avg_score90), avg_score90[23], avg_score90[10]),
       pch = 21, col = "black", bg = "red")
text(x = c(which.min(avg_score90)-7, 8.5, which.max(avg_score90)+6, 26, 10),
     y = c(min(avg_score90), avg_score90[5], max(avg_score90), avg_score90[23], avg_score90[10]+0.05),
     labels = c("Souls Of The Departed", "Better Days", "My Beautiful Reward", "Real Man", "Gloria's Eyes"))
```
Such as the previous decade, the 1990s average sentiment scores are generally positive, with very few songs being labeled as negative. The lowest result is obtained by *Souls of The Departed*.  
  
### 3.4 The 2000s
  
```{r 2000s_importing_and_cleaning, message=FALSE, warning=FALSE}
setwd("~/songs/decades/2000s")
names00 <- list.files()
text00 <- lapply(names00, readLines)
Lyrics00 <- Corpus(VectorSource(text00))
Lyrics00_lemmatized <- clean_lyrics(Lyrics00, lemmatize = TRUE, my_sw = mystopwords)
```

```{r tdm_2000}
# Lemmatized Text
Lyrics00_dtm <- TermDocumentMatrix(Lyrics00_lemmatized)
dtm_m <- as.matrix(Lyrics00_dtm)
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm00_lm <- data.frame(word = names(dtm_v),freq=dtm_v)
```

```{r wordclouds_00s, fig.cap="Figure 11: 2000s Lemmatized Word Cloud", warning=FALSE, fig.width=5, fig.align="center"}
cols <- brewer.pal(9, 'YlGn')
newcol <- colorRampPalette(cols)
ncols <- 600
cols2 <- newcol(ncols)
cols2 <- cols2[580:1]
wc <- wordcloud2(dtm00_lm, fontFamily = "Arial", size = 1.5, color = cols2)
saveWidget(wc,"wordclouds/00.html", selfcontained = F)
webshot("wordclouds/00.html", "wordclouds/00.png", vwidth = 960, vheight = 628, delay = 15)
```

```{r sentimet_score00s, out.width="75%", fig.align="center", fig.cap="Figure 12: Average Sentiment Score for each song from the 2000s"}
avg_score00 = numeric(0)
lyrics_score00 = list()
for(i in 1:length(text00)){
  lyrics_score00[[i]] <- get_sentiment(text00[[i]][text00[[i]]!=""], method="syuzhet")
  avg_score00[i] <- mean(lyrics_score00[[i]])
}
plot(avg_score00,
     type = "l", ylab = "Average Sentiment Score", xlab = "",
     main = "Songs from 2000-2009")
abline(h=0, col="red")
points(x = c(which.min(avg_score00), which.max(avg_score00), 42, 18),
       y = c(min(avg_score00), max(avg_score00), avg_score00[42], avg_score00[18]),
       pch = 21, col = "black", bg = "red")
text(x = c(which.min(avg_score00)-7, which.max(avg_score00)-10, 56, 18),
     y = c(min(avg_score00), max(avg_score00), avg_score00[42], avg_score00[18]+0.07),
     labels = c("Last To Die", "Surprise, Surprise", "O Mary Don't You Weep",
                "I'll Work For Your Love"))
```
After 2 decades of mainly positive lyrics, the 2000s songs show a more even distribution around zero. *Surprise, Surprise* takes the crown of the most positive song of the decade (in fact, it is the most positive of Springsteen's whole discography, with an average score of 0.899), whereas *Last To Die* results as the most negative song.

### 3.5 The 2010s
  
```{r 2010s_importing_and_cleaning, message=FALSE, warning=FALSE}
setwd("~/songs/decades/2010s")
names10 <- list.files()
text10 <- lapply(names10, readLines)
Lyrics10 <- Corpus(VectorSource(text10))
Lyrics10_lemmatized <- clean_lyrics(Lyrics10, lemmatize = TRUE, my_sw = mystopwords)
```

```{r tdm_2010}
# Lemmatized Text
Lyrics10_dtm <- TermDocumentMatrix(Lyrics10_lemmatized)
dtm_m <- as.matrix(Lyrics10_dtm)
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm10_lm <- data.frame(word = names(dtm_v),freq=dtm_v)
```

```{r wordclouds_10s, fig.cap="Figure 13: 2010s Lemmatized Word Cloud", warning=FALSE, fig.width=5, fig.align="center"}
cols <- brewer.pal(9, 'Oranges')
newcol <- colorRampPalette(cols)
ncols <- 600
cols2 <- newcol(ncols)
cols2 <- cols2[560:1]
wc <- wordcloud2(dtm10_lm, fontFamily = "Arial", size = 1.2, color = cols2)
saveWidget(wc,"wordclouds/10.html", selfcontained = F)
webshot("wordclouds/10.html", "wordclouds/10.png", vwidth = 960, vheight = 628, delay = 12)
```

```{r sentimet_score10s, out.width="75%", fig.align="center", fig.cap="Figure 14: Average Sentiment Score for each song from the 2010"}
avg_score10 = numeric(0)
lyrics_score10 = list()
for(i in 1:length(text10)){
  lyrics_score10[[i]] <- get_sentiment(text10[[i]][text10[[i]]!=""], method="syuzhet")
  avg_score10[i] <- mean(lyrics_score10[[i]])
}
plot(avg_score10, type = "l", ylab = "Average Sentiment Score", xlab = "",
     main = "Songs from 2010-2020")
abline(h=0, col="red")
points(x = c(which.min(avg_score10), which.max(avg_score10), 76, 17, 10),
       y = c(min(avg_score10), max(avg_score10), avg_score10[76], avg_score10[17], avg_score10[10]),
       pch = 21, col = "black", bg = "red")
text(x = c(which.min(avg_score10)+5.5, which.max(avg_score10)+5, 59, 17, 18),
     y = c(min(avg_score10), max(avg_score10), avg_score10[76], avg_score10[17]-0.07, avg_score10[10]),
     labels = c("Stones", "Baby I", "We Take Care of Our Own",
                "Drive Fast (The Stuntman)", "Candy's Boy"))
```
In Springsteen's latest (and most productive) decade of his career, the average sentiment distribution appears to be positive, although *Stones* is actually labeled as the most negative across his whole discography.

***

## 4. Decade Comparison

Let us now compare the results of the separate decades.

```{r decades_structure}
nsongs <- c(length(Lyrics70), length(Lyrics80), length(Lyrics90),
            length(Lyrics00), length(Lyrics10))
twords <- c(sum(dtm70_lm[,2]), sum(dtm80_lm[,2]), sum(dtm90_lm[,2]), sum(dtm00_lm[,2]), sum(dtm10_lm[,2]))
uwords <- c(nrow(Lyrics70_dtm), nrow(Lyrics80_dtm), nrow(Lyrics90_dtm),
            nrow(Lyrics00_dtm), nrow(Lyrics10_dtm))
wps <- round(twords / nsongs, 2)
uwps <- round(uwords / nsongs, 2)
names(nsongs) <- c("1970-1979", "1980-1989", "1990-1999", "2000-2010", "2010-2020")
kable(cbind(nsongs, twords, uwords, wps, uwps), col.names = c("Number of Songs", "Total Lemmas", "Unique Lemmas", "Lemmas per Song", "Unique Lemmas per Song"),
      caption = "Table 5: Count and average length of Springsteen's Lyrics in each decade", align = "c") %>%
  kable_styling(full_width = F)
```
Going forward with the years, there seems to be a trend towards producing a greater amount of songs, but which are overall shorter and with fewer unique terms. In fact, whereas the 1970s were Springsteen's least prolific years, the songs produced in this decade contain on average ~30 lemmas more than those from any other decade. The 2010s, on the other hand, are characterized by an average of 20.9 unique lemmas per song, which is the shortest among all considered periods.  
  
The word clouds have already highlighted the lyrics semantic differences and analogies among the decades. However, a deeper analysis of the most common terms used can be conduced by computing the *frequency of use* of these words, as the ratio of their count in the Term-Document Matrix among the total number of lemmas for each decade (second column of *Table 5*).

```{r mostcommon_words}
dtms <- list(dtm70_lm, dtm80_lm, dtm90_lm, dtm00_lm, dtm10_lm)
output <- list()
mat <- NULL
for(i in 1:length(dtms)){
  words <- dtms[[i]][1:10, 1]
  freq <- dtms[[i]][1:10, 2]/sum(dtms[[i]][,2])
  perc <- paste(round(freq*100, 2), "%")
  output[[i]] <- cbind(words, perc)
  mat <- cbind(mat, output[[i]])
}
kable(mat, col.names = c("1970s", "", "1980s", "", "1990s", "", "2000s", "", "2010s", ""),
      caption = "Table 6: Most frequent lemmas for each decade", align = "c") %>%
  kable_styling(full_width = F)
```

```{r some_common_words, fig.align="center", out.width="80%", fig.cap="Figure 15: Frequency evolution of some of the most common lemmas over each decade", fig.asp=.5}
word_evolution <- function(words, dtms, years, plot_title, color_pal = "Set2"){
  percs <- list()
for(i in 1:length(words)){
  perc <- numeric(0)
  for(j in 1:length(dtms)){
    perc[j] <- dtms[[j]][words[i], 2]*100/sum(dtms[[j]][,2])
  }
  percs[[i]] <- perc
}
names(percs) <- words
df <- NULL
for(i in 1:length(words)){
  mat <- cbind(decades, words[i], percs[[i]])
  df <- rbind(df, mat)
}
df <- as.data.frame(df)
colnames(df) <- c("decade", "word", "perc")
df$perc <- as.numeric(df$perc)
ggplot(data = df, aes(x = decade, y = perc, group = word, col = word)) +
  geom_line(size = 1) + labs(x = "Decade", y = "% over the total number of words",
                             title = plot_title, color = "Lemma") +
  scale_color_brewer(palette=color_pal) + theme_bw()
}

words <- c("good", "baby", "night", "come", "love", "man")
dtms <- list(dtm70_lm, dtm80_lm, dtm90_lm, dtm00_lm, dtm10_lm)
decades <- c("1970s", "1980s", "1990s", "2000s", "2010s")
title = "Evolution of the most used lemmas"

word_evolution(words, dtms, decades, title)
```
The only lemma which is always above 1% is *good*, with a peak over 2% between the 1980s and the 1990s. The use of *baby* appears to alternate among decades, with the 1980s and 2010s as the periods of greater frequency. *Come* and *love* have a very similar increasing path over time, although it has slowed down in the last decade. On the other hand, *man* was a quite common lemma until the 1990s, but has dropped below 0.5% ever since. Finally, *night* shows a very interesting trend: it was the most common term in Springsteen's early years, but it suffered a steep downfall in frequency of use until the last decade, when it went back above 1%.

```{r interesting_words, fig.align="center", out.width="80%", fig.cap="Figure 16: Frequency evolution of other interesting terms", fig.asp=.5}
words <- c("death", "life", "dream", "job")
title <- "Evolution of other interesting lemmas"

word_evolution(words, dtms, decades, title, color_pal = "Set1")
```
Figure 16 shows the evolution of some other interesting (not necessarily common) terms: *life*, *death*, *job*, *dream*.  
*Death*, while being always below 0.2%, has definitely become more common in the latest years of Springsteen's career, whereas *life* shows a generally decreasing path, except the peak in the 2000s which is probably due to *This Life* and *Life Itself* (both from *Working On A Dream*, 2009).  
The frequency of *job* appears to be increasing until the 1980s, but this term has been almost ignored over the past 2 decades. *Dream*, on the contrary, shows a steady increase ever since the 1990s, and has become one of the most frequent lemmas in recent years.

```{r sentiment_distribution, fig.cap="Figure 17: Average Sentiment Score distribution for each decade", out.width="75%", fig.align="center"}
boxplot(avg_score70, avg_score80, avg_score90, avg_score00, avg_score10,
        col=brewer.pal(5, "Set3"),
        names=c("1970s", "1980s", "1990s", "2000s", "2010s"),
        main="Sentiment Score Distribution for each decade")
abline(h=0, col="red", lwd=2, lty=2)
```
Finally, Figure 17 shows the distributions of the Average Sentiment Scores of the songs from each decade. As already stated when analyzing the general boxplot (Figure 4), most of the lyrics are on average positive, and every decade has a median score above zero. In particular, the 1970s appear as the most balanced decade, whereas the other boxplots are clearly more shifted towards 1. The 1980s look very close to the 2010s in terms of average Sentiment Score location and scale, while the 1990s and the 2000s have a narrower distribution. 
  
***

## Conclusions

Springsteen's lyrics have surely evolved throughout the years for what concerns the main themes and the lexical complexity. Of course, this is to be expected when we are talking about an almost 50-years career. However, some key points such as *love* and *good* appear to have remained stable in his verses, such has his tendency to write songs which convey positive feelings. Indeed, the average sentiment score of his lyrics seems distributed quite equivalently across the decades, and overall his discography the emotions of *joy*, *trust*, *anticipation* and *surprise* appear consistently more than those of *sadness*, *fear*, *anger* and *disgust*.  
Overall, the sentiment scores given by the *Syuzhet* dictionary appeared quite coherent with the actual lyrics meaning, even though sentences which expressed emotions through irony or other rhetorical devices were (as expected) difficult to deal with.

***

## References
  
- Matthew Jockers, 2020-11-24. *Introduction to the Syuzhet Package* (https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html)
- Sanil Mhatre, 2020-05-13. *Text Mining and Sentiment Analysis: Analysis with R* (https://www.red-gate.com/simple-talk/sql/bi/text-mining-and-sentiment-analysis-with-r/)
- Ryan Mitchell, 2018. *Web Scraping with Python*, 2nd ed. O'Reilly.
- Al Sweigart, 2019. *Automate The Boring Stuff With Python*, 2nd ed. No Starch Press.
- CristÃ³bal Veas, 2020-09-25. *How to Analyze Emotions and Words of the Lyrics From your Favorite Music Artist* (https://towardsdatascience.com/how-to-analyze-emotions-and-words-of-the-lyrics-from-your-favorite-music-artist-bbca10411283)

***  

- GitHub Repository with the function used for web scraping: https://github.com/a-montanari/webscraping_lyrics
- Lyrics downloaded from AZLyrics (https://www.azlyrics.com)
